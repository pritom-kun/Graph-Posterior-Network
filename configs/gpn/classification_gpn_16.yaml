run:
  job: train
  eval_mode: default
  experiment_directory: ./saved_experiments
  save_model: true
  
  # if you don't want to run experiments on a GPU, set this flag to false
  gpu: 0 # or false
  
  experiment_name: classification
  num_inits: 10

data:
  
  # change this to get a different random split with any (split_no > 0)
  split_no: 1
  
  # change this for a different dataset
  dataset: CoraML 
  
  root: ./data
  ood_flag: false
  train_samples_per_class: 0.05
  val_samples_per_class: 0.15
  test_samples_per_class: 0.8
  split: random


model:
  
  # change this to a get a different split and random model initialization at the same time
  seed: 42
  
  # change this to a get a different random model initialization (init_no > 0)
  init_no: 1
  
  model_name: GPN
  dim_hidden: 64
  dropout_prob: 0.5
  K: 10
  add_self_loops: true
  maf_layers: 0
  gaussian_layers: 0
  use_batched_flow: true
  loss_reduction: sum
  approximate_reg: true
  flow_weight_decay: 0.0
  pre_train_mode: flow
  alpha_evidence_scale: latent-new
  alpha_teleport: 0.1
  entropy_reg: 0.00001 
  dim_latent: 16
  radial_layers: 10
  activation_type: 'ReLU'

  lipschitz_reg: 0.000
  lipschitz_init: 100000.0

  stress_reg: 0.00
  stress_use_graph: false
  stress_metric: 'cosine'
  stress_knn_k: 5
  stress_scaling: 'linear'
  stress_row_normalize: false
  stress_drop_last_N: 0
  stress_drop_orthog: false
  stress_sym_single_dists: false
  stress_force_connected: 'true-random'

  # fixed_point_loss: 1.0
  decoder_reg: 0.01
  # LDA_loss: 1.0
  # nll_loss: 1.0
  uce_loss: 1.0
  # mse_loss: 1.0
  # mae_loss: 1.0
  # entropy_reg: 0.00001

  dist_perserving: False
  dist_embedding_beta: 'dirichlet-evidence'
  # dist_embedding_beta: 'preflow-l2'

  # orig_dist_reg: 0.0001
  # dist_reg: 0.00000001
  dist_sigma: 5
  KNN_K: 5
  knn_mode: 'knn'
  save_tsne: true

training:
  epochs: 100000
  stopping_mode: default
  stopping_patience: 50
  stopping_restore_best: true
  stopping_metric: val_CE
  stopping_minimize: true
  finetune_epochs: 0
  warmup_epochs: 5
  lr: 0.01
  weight_decay: 0.001
